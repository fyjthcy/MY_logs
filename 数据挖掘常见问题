1.训练集小的时候用什么分类算法？
答：高方差低偏差的分类器，朴素贝叶斯，比低偏差高方差的分类器，如k近邻或logistic回归更有优势，后者容易过拟合。训练集多，则相反。。。
2.数据和算法之间的关系？
答：数据集足够大，算法的区别并不大。对于特征的提取是更重要的一点，如果很在意准确率，可以尝试将不同分类器进行合并，构成新的模型。
3.常见机器学习方法的特点？
答：

朴素贝叶斯：
1).feature长度不同，要归一化为相同长度。
p(ci/w)=p(w/ci)p(ci)/p(w);
优点：小规模数据表现良好，适合多分类，适合增量式训练。
缺点：数据表达形式敏感。

logistic回归：
应用最大似然估计实现的线性分类器
优点：
实现简单 计算量小 速度快 存储资源低
缺点：
容易欠拟合 准确度不高 只可二分类 只可线性 

线性回归：
真正的回归方法，不是分类
实现简单 但不能拟合非线性数据

KNN算法：
优点：思想简单 理论成熟 可分类可回归 可用于非线性 复杂度O(n) 准确度高 对outlier不敏感
缺点：计算量大 样本不平衡 需大量内存

SVM：
优点：可用于线性/非线性分类，也可以用于回归；低泛化误差；容易解释；计算复杂度较低；
缺点：对参数和核函数的选择比较敏感；原始的SVM只比较擅长处理二分类问题；

决策树：
理解熵，信息增益的关系
优点：解释性强 比较适合确实属性的样本 能够处理不相关的特征
缺点：容易过拟合 随机森林比他要好

boosting:
准确率较高 没太多参数可调节
但对outlier比较敏感

adaboost:
优点：
1.提供框架，框架内有子分类器，可使用简单的弱分类器，不用对特征进行筛选，不存在过拟合。
2.不需要若分类器先验知识，得到的强分类器的分类精度依赖于所有弱分类器。无论应用于人类数据还是真实数据，它都能提高学习精度。
3.可以根据弱分类器的反馈，自适应的调整假定的错误率，执行效率很高。
4.结合几个弱分类器，只要弱分类器达到一定的优势，强分类器的错误率将趋近于0。
缺点：
1.容易受噪声的影响，分类器的选择是个问题，训练时间偏长。
2.比较难分的数据，权重会指数增长，训练会偏向这类样本，左右误差的计算，降低分类精度。

k-means：
优点：
（1）k-means算法是解决聚类问题的一种经典算法，算法简单、快速。
（2）对处理大数据集，该算法是相对可伸缩的和高效率的，因为它的复杂度大约是O(nkt)，其中n是所有对象的数目，k是簇的数目,t是迭代的次数。通常k<<n。这个算法通常局部收敛。
（3）算法尝试找出使平方误差函数值最小的k个划分。当簇是密集的、球状或团状的，且簇与簇之间区别明显时，聚类效果较好。
缺点：
（1）k-平均方法只有在簇的平均值被定义的情况下才能使用，且对有些分类属性的数据不适合。
（2）要求用户必须事先给出要生成的簇的数目k。
（3）对初值敏感，对于不同的初始值，可能会导致不同的聚类结果。
（4）不适合于发现非凸面形状的簇，或者大小差别很大的簇。
（5）对于”噪声”和孤立点数据敏感，少量的该类数据能够对平均值产生极大影响。

GDBT：

GBDT(Gradient Boosting Decision Tree) 又叫 MART（Multiple Additive Regression Tree)，
好像在阿里内部用得比较多（所以阿里算法岗位面试时可能会问到），它是一种迭代的决策树算法，
该算法由多棵决策树组成，所有树的输出结果累加起来就是最终答案。
它在被提出之初就和SVM一起被认为是泛化能力（generalization)较强的算法。
近些年更因为被用于搜索排序的机器学习模型而引起大家关注。
GBDT是回归树，不是分类树。
GBDT的核心就在于，每一棵树学的是之前所有树结论和的残差，这个残差就是一个加预测值后能得真实值的累加量。
为了防止过拟合，和Adaboosting一样，也加入了boosting这一项

4.常见归一化方法？
答：
1)min-max标准化：x=(x-min)/(max-min);
2)X-score标准化：x=(x-u)/p; u为均值，p为标准差；
